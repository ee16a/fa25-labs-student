{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"otter":{"OK_FORMAT":true,"tests":{"q1a":{"name":"q1a","points":null,"suites":[{"cases":[{"code":">>> assert np.allclose(centered_magnitude_spectrum(np.ones(10)), np.array([0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 0.0]))\n","hidden":false,"locked":false},{"code":">>> assert np.allclose(centered_magnitude_spectrum([10] * 5 + [6 * 2] + [3] * 4), np.array([2.0, 9.0, 5.46491887, 9.0, 21.6364198, 74.0, 21.6364198, 9.0, 5.46491887, 9.0]))\n","hidden":false,"locked":false},{"code":">>> x = np.array([0.77498361, 0.02466649, 0.5159193, 0.75489558, 0.85777587, 0.56253859, 0.01691631, 0.65058933, 0.4358812, 0.80630942])\n>>> assert np.allclose(centered_magnitude_spectrum(x), np.array([0.19752313, 1.15615072, 0.59527128, 1.55793533, 0.21334073, 5.4004757, 0.21334073, 1.55793533, 0.59527128, 1.15615072]))\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q2a":{"name":"q2a","points":null,"suites":[{"cases":[{"code":">>> assert isinstance(freq_indices, np.ndarray), 'freq_idx has a type that is not np.ndarray'\n>>> assert isinstance(time_indices, np.ndarray), 'time_idx has a type that is not np.ndarray'\n>>> assert np.allclose(freq_indices[10:20], np.array([20, 22, 22, 24, 27, 40, 40, 45, 45, 53]))\n>>> assert np.allclose(time_indices[30:40], np.array([132, 597, 784, 311, 421, 454, 357, 637, 271, 245]))\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q2b":{"name":"q2b","points":null,"suites":[{"cases":[{"code":">>> test_fs, test_coldplay = wavfile.read('VivaLaVida.wav')\n>>> assert isinstance(fingerprint(test_fs, test_coldplay), list), 'fingerprint returned data of a type that is not list'\n>>> assert np.allclose([pr[1] for pr in fingerprint(test_fs, test_coldplay)[100:120]], [3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667])\n>>> assert np.allclose([pr[1] for pr in fingerprint(test_fs, test_coldplay, neighborhood_size=21)[100:120]], [0.864, 0.864, 0.864, 0.864, 0.864, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334, 1.2373333333333334])\n>>> assert np.allclose([pr[1] for pr in fingerprint(test_fs, test_coldplay, amp_thresh=25)[100:120]], [3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 3.8506666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667, 4.746666666666667])\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q3a":{"name":"q3a","points":null,"suites":[{"cases":[{"code":">>> test_fs, test_coldplay = wavfile.read('VivaLaVida.wav')\n>>> coldplay_20 = get_20_second_segment(test_fs, test_coldplay)\n>>> assert isinstance(coldplay_20, np.ndarray), 'function returned a type that was not np.ndarray'\n>>> assert len(coldplay_20) == 20 * test_fs, 'coldplay segment was not 20 seconds'\n>>> assert np.isin(coldplay_20, test_coldplay).any(), 'coldplay segment content was not contained within coldplay audio'\n","hidden":false,"locked":false},{"code":">>> test_fs, test_killers = wavfile.read('MrBrightside.wav')\n>>> killers_20 = get_20_second_segment(test_fs, test_killers)\n>>> assert len(killers_20) == 20 * test_fs, 'killers segment was not 20 seconds'\n>>> assert np.isin(killers_20, test_killers).any(), 'killers segment content was not contained within killers audio'\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q3b":{"name":"q3b","points":null,"suites":[{"cases":[{"code":">>> test_fs, test_coldplay = wavfile.read('VivaLaVida.wav')\n>>> test_fs, test_killers = wavfile.read('MrBrightside.wav')\n>>> assert isinstance(basic_detect_test(test_fs, test_killers), tuple), 'basic_detect_test returned a type that was not tuple'\n>>> assert basic_detect_test(test_fs, test_killers) == ('MrBrightside.wav', 100.0), 'killers was not identified or confidence was not 100%'\n>>> assert basic_detect_test(test_fs, test_coldplay) == ('VivaLaVida.wav', 100.0), 'coldplay was not identified or confidence was not 100%'\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"q3c":{"name":"q3c","points":null,"suites":[{"cases":[{"code":">>> test_fs, test_coldplay = wavfile.read('VivaLaVida.wav')\n>>> test_fs, test_killers = wavfile.read('MrBrightside.wav')\n>>> killers_fn, killers_confidence = gaussian_noise_detect_test(test_fs, test_killers)\n>>> assert killers_fn == 'MrBrightside.wav', 'killers was not identified'\n>>> assert killers_confidence > 50, 'killers confidence is less than 50%'\n>>> coldplay_fn, coldplay_confidence = gaussian_noise_detect_test(test_fs, test_coldplay)\n>>> assert coldplay_fn == 'VivaLaVida.wav', 'coldplay was not identified'\n>>> assert coldplay_confidence > 50, 'coldplay confidence is less than 50%'\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Initialize Otter\nimport otter\ngrader = otter.Notebook(\"eecs16a_shazam.ipynb\")","metadata":{"deletable":false,"editable":false,"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EECS 16A Shazam\n### EECS 16A: Foundations of Signals, Dynamical Systems, and Information Processing, Fall 2025\n\nTaken and modified from EE 120: Signals and Systems at UC Berkeley\n\nAcknowledgements:\n\n- **Spring 2020** (v1.0): Anmol Parande, Dominic Carrano, Babak Ayazifar\n- **Spring 2022** (v2.0): Anmol Parande\n- **Spring 2023** (v2.1): Yousef Helal\n- **Fall 2023** (v2.2): Christine Zhang\n- **Fall 2024** (v3.0): Nikhil Ograin\n- **Spring 2025** (v3.1): Sonia Chacon\n- **Fall 2025** (v3.2): Andrew Song","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"markdown","source":"# Background\n\nIn 2002, Shazam Entertainment Limited (founded by UC Berkeley students!) launched its music identification product, allowing users to dial a phone number and play a song. Then, they'd get a text message with the name of the song and its artist. In 2018, Shazam was acquired by Apple for \\$400 million, and it's now in every iPhone.\n\nShazam works by using *audio fingerprinting*: given a song, it generates a set of identifiers, and searches an audio database to find a match and identify the song. In this lab, you'll learn about audio fingerprinting, and use it to build a music identification just like Shazam!\n\n## Dependencies\n\nTo get started, you will need to import these dependencies which are already installed on Datahub.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.signal as signal\nimport pandas as pd\nimport IPython.display as ipd\n\nfrom scipy.io import wavfile\nfrom scipy.ndimage import maximum_filter\nfrom shazam_utils import hashing\n\nimport otter\ngrader = otter.Notebook(\"eecs16a_shazam.ipynb\")","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Note*: To avoid any copyright issues, we've cropped all provided songs to only contain the first 60 seconds.","metadata":{}},{"cell_type":"markdown","source":"### Python Imports Primer\nIn Python Bootcamp, we introduced NumPy and some other Python libraries. In this lab, you'll have to use some functions from the libraries imported above. But wait; what's an import? And how do you use any of these?\n\n#### `import`\nAn import statement in Python indicates that you can use the functions from that Python module (a fancy way of saying file) within the current file. Your current file is this Jupyter notebook! So, a basic example of an import statement would be\n```\nimport numpy\n```\nThis lets us use NumPy functions such as `numpy.array` or `numpy.eye`. However, for some packages (such as NumPy, Pandas, or Matplotlib) it is standard practice to import them as another name. For example:\n```\nimport numpy as np\n```\nThis still lets us use NumPy functions, but using `np.FUNCTION` instead of `numpy.FUNCTION`. For example, `np.array` or `np.eye`. Note that we have done this for you above.\n\nExtensions on imported modules work in exactly the same way but do NOT require further imports. For example `np.linalg.solve` only requires the import statement `import numpy as np`.\n\nImport statements only need to be defined once! Meaning they should not be contained within functions (that are by design intended to run multiple times).\n\n#### `from`\nIf we want to only import a specific function (or set of functions), we can use the `from` keyword. For example, if we want `numpy.array`:\n```\nfrom numpy import array\n```\nWe can then use this without the `numpy` prefix, simply `array`. We can combine this with the `as` keyword as well:\n```\nfrom numpy import array as arr\n```\nThe `numpy.array` function is then usable as `arr`.","metadata":{}},{"cell_type":"markdown","source":"## Glossary\n\nWe know there are a lot of (probably new) acronyms which this lab will introduce, so we've included a brief listing of them here:\n\n- CT: Continuous Time - time progresses continuously, i.e. time which spans all real numbers within some interval, how we live in the real life world\n- DT: Discrete Time - time takes discrete steps and is undefined for all other real numbers within some interval, i.e. when data is sampled at a certain rate\n- FT: Fourier Transform - an mathematical operation that decomposes a time-domain signal or function into frequencies\n- DFT: Discrete Fourier Transform - a Fourier transform that takes in a discrete time-domain signal and outputs a discrete frequency-domain signal\n- FFT: Fast Fourier Transform - an optimized algorithm for performing a DFT (usually computerized)\n- DTFT: Discrete Time Fourier Transform - a Fourier transform that takes in a discrete time-domain signal and outputs a continuous frequency-domain signal\n\nYou don't need to understand these right now! However, if you ever find yourself stuck on a vocabulary term while completing this lab, you can come back to these definitions. Now onto the lab content!","metadata":{}},{"cell_type":"markdown","source":"# Q1: Spectral Analysis\n\nFor many types of data, the constituent frequencies of a signal tell us a lot about it. The same is true of audio: to find the salient features of songs to fingerprint, we'll need to look at the song's spectrum (i.e., Fourier Transform). Fortunately, we have the DFT (efficiently implemented via the FFT) to help us do this.\n\nTo get started, let's load in *Viva La Vida* by Coldplay.","metadata":{}},{"cell_type":"markdown","source":"Typically, we think of audio as a two-channel, continuous signal $\\vec{x}(t) = \\left[x_L(t) \\ x_R(t)\\right]$, with one column of the *audio matrix* per channel. That is, $x_L(t)$ is the left channel's signal, and $x_R(t)$ the right channel's signal. The reason we have two distinct audio channels is so that we can have two streams playing at the same time, one per ear (e.g., in a pair of headphones or laptop speakers).\n\nWe sample this continuous-time (CT) audio signal at a particular rate (here, 48000 Hz) to get a discrete-time (DT) signal. For our purposes, the distinction between our channels is not very important, so we'll just average them to form a 1D signal, $x(n)$.","metadata":{}},{"cell_type":"code","source":"fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\nprint(f\"Audio Shape: {coldplay.shape}, Sampling Rate: {fs} Hz\")\ncoldplay = np.mean(coldplay, axis=1)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To show you what this looks like in the time-domain (i.e. before we have performed the DFT on this data), execute the following code block.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 4), dpi=200)\ncoldplay_labels = [x / fs for x in range(fs*10)]\nplt.plot(coldplay_labels, coldplay[:fs*10])\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Magnitude\")\nplt.title(\"Time-domain visualization of first 10 seconds of Viva La Vida\")\nplt.show()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To get a sense for the song we're working with, feel free to have a listen! This cell may take a few seconds to run.","metadata":{}},{"cell_type":"code","source":"ipd.Audio(\"VivaLaVida.wav\")","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Q1a: One DFT is Not Enough\n\nAs far as spectral analysis is concerned, it seems like we should just be able to take the DFT of the entire song, find our fingerprints, and be done, right? Is that really all there is to Shazam? No, not quite. It may not be obvious, but there's a big issue with this approach that we'll explore now. So that our code doesn't take forever to run, we'll only look at the first 10 seconds of the song, but the issues we'll find here apply generally to the entire signal.\n\nTo start, let's define a function which will give us the magnitude spectrum of the signal $|X(\\omega)|$ centered around $\\omega = 0$. \n\n### Your Job\n\nFill in the code for `centered_magnitude_spectrum`, which takes in a signal and outputs its centered magnitude spectrum.\n\nYour function should do the following:\n1. Perform a 1-D DFT on `sig`\n2. Shift the DFT to $[-\\pi$, $\\pi]$\n3. Find the magnitude of the shifted DFT, i.e. $|X(\\omega)|$\n\nWhy is this function named `centered_magnitude_spectrum`? Let's break it down into components:\n- `centered`: By default, when you compute the FFT, the samples of the DFT that are returned go from $0$ to $2\\pi$; centering them so that they go from $-\\pi$ to $\\pi$ is nicer for visualization.\n- `magnitude`: The signal is composed of complex numbers, and we only really care about their magnitudes.\n- `spectrum`: A \"spectrum\" is a fancy way of saying \"some data over a range of frequencies\", which is what a DFT outputs.\n\n*Hint*: Check out [np.fft.fft](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html) to take the DFT of a discrete signal and [np.fft.fftshift](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftshift.html) to center your spectrum.","metadata":{}},{"cell_type":"code","source":"def centered_magnitude_spectrum(sig):\n    \"\"\"\n    Inputs:\n    sig - a generic iterable signal of floating point numbers\n    \n    Output (np.ndarray):\n    Returns a centered magnitude spectrum of the given signal. \n    That is, the magnitude of the DFT of the provided signal \n    after shifting from [0,2pi] to [-pi,pi].\n    \"\"\"\n    \n    # TODO YOUR CODE HERE\n\n    ...","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grader.check(\"q1a\")","metadata":{"deletable":false,"editable":false,"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To see why one DFT won't suffice, we're going to look at the spectrum of different sections of Viva La Vida.\n\nFirst, we'll look at magnitude spectrum of the first 10 seconds of the song.","metadata":{}},{"cell_type":"code","source":"coldplay_cropped = coldplay[:10 * fs]\ncoldplay_freqs = centered_magnitude_spectrum(coldplay_cropped)\n\nplt.figure(figsize=(16, 4), dpi=200)\nfreqs = np.linspace(-fs/2, fs/2, len(coldplay_freqs))\nplt.plot(freqs, coldplay_freqs)\nplt.xlabel(\"Frequency [Hz]\")\nplt.ylabel(\"Magnitude\")\nplt.title(\"DFT of first 10 seconds of Viva La Vida\")\nplt.show()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most of the frequency content is centered around the lower frequencies. In fact, we can barely see anything past 10 kHz, because human hearing stops around 15-20 kHz (and generally decreases with age), so there's no reason to include anything that high in music.","metadata":{}},{"cell_type":"markdown","source":"So far, everything looks ok: we got the spectrum of the first 10 seconds of our song. This gives us a sort of \"aggregate view\" of the frequencies that show up at some point during the first 10 seconds. But is this \"aggregate view\" good enough? What happens if our signal is *non-stationary*, i.e its frequency content changes with time, as is certainly the case with music? \n\nTo find out, let's look at the magnitude spectra of the first, second, third, and fourth seconds of the song. We'll use these to zoom in (temporally speaking) and inspect the song's frequency content over the course of a second of data (rather than 10), and see if the \"aggregate view\" gives a good enough picture of what frequencies are present at a specific second in time.","metadata":{}},{"cell_type":"code","source":"coldplay_freqs_1 = centered_magnitude_spectrum(coldplay[:fs]) \ncoldplay_freqs_2 = centered_magnitude_spectrum(coldplay[fs:2*fs])\ncoldplay_freqs_3 = centered_magnitude_spectrum(coldplay[2*fs:3*fs])\ncoldplay_freqs_4 = centered_magnitude_spectrum(coldplay[3*fs:4*fs])\n\nfreqs = np.linspace(-fs / 2, fs / 2, len(coldplay_freqs_1))\nsigs = [coldplay_freqs_1, coldplay_freqs_2, coldplay_freqs_3, coldplay_freqs_4]\nstrs = [\"1st\", \"2nd\", \"3rd\", \"4th\"]\n\nplt.figure(figsize=(16, 10), dpi=200)\nfor i in range(1, 5):\n    plt.subplot(2, 2, i)\n    plt.plot(freqs, sigs[i-1])\n    plt.xlim([-.5e4, .5e4])\n    plt.ylim([0, 1.1 * np.array(sigs).max()])\n    plt.title(\"DFT magnitude of {} second of Viva La Vida\".format(strs[i-1]))\nplt.show()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Notice how while most of the energy in each second's spectrum is concentrated inside $[-2.5 \\text{ kHz}, +2.5 \\text{ kHz}]$, the exact shapes are quite different. \n\n**The issue is that the aggregate view from our 10-second DFT doesn't have good enough *temporal resolution*: we can't see how the signal's frequency content changes over time!**\n\nWhy does this matter, you ask? Well, when we're working with the real deal, we don't feed Shazam the entire song; only a clip. For example, suppose you tune into a radio station halfway through a song. Then, 20 seconds later, you think to yourself, \"hey, I like this\" and pull out Shazam to figure out what song it is. By then, whatever you're giving Shazam is missing a lot of data, and so it needs to be able to look at what frequencies are in the song at different points in time to correctly identify it. The aggregate view won't do. Fortunately, there's a very simple fix to this.","metadata":{}},{"cell_type":"markdown","source":"## Q1b: Spectrogrammin'\n\nThe results of Q1a are pretty clear: we need a way to see how the signal's frequency content changes over time. Just taking one DFT of the entire signal fails to achieve this. Instead, we'll use a *spectrogram*.","metadata":{}},{"cell_type":"markdown","source":"### Spectrograms\n\nA *spectrogram* is an image representing the frequency content of a signal at different times. This ability to see how a signal's frequency content changes with time is the key useful feature of a spectrogram. \n\nTo compute a spectrogram, we split our signal into chunks, compute the DFT of each chunk, and plot the magnitude squared of those DFT chunks side-by-side. To make visualization easier, we typically employ a colormap to distinguish where the DFT's squared-magnitude is bigger.\n\nFor example, here is a spectrogram of speech, taken from [here](https://www.researchgate.net/figure/Spectrogram-of-a-speech-signal-with-breath-sound-marked-as-Breath-whose-bounds-are_fig1_319081627). The red areas correspond to stronger frequency content, and green areas to weaker frequency content.\n\nNotice the differences between when the speaker takes a breath and when the speaker is actually speaking. A single DFT wouldn't be able to separate this!\n\n![speech-spectrogram.png](speech-spectrogram.png)","metadata":{}},{"cell_type":"markdown","source":"Remember that this spectrogram isn't new data. It is simply a new view of the existing time-domain data we already have. The image below ([source](https://www.tek.com/de/blog/spectrogram-types-the-many-faces-of-the-spectrogram)) shows this in a visual form.\n![spectrogram_display.png](spectrogram_display.jpg)","metadata":{}},{"cell_type":"markdown","source":"To get some familiarity with spectrograms, let's generate some sinusoidal signals and plot their spectrograms. You don't need to write any code for this question. Just run the cells!\n\nIn the cell below, we generate 1000 samples of the following signals for one second (i.e. over the interval $t \\in [0, 1]$):\n- A 100 Hz sine wave (call it `x1`).\n- A 400 Hz sine wave (call it `x2`).\n- A third signal, call it `x3`, by concatenating `x1` and `x2`. ","metadata":{}},{"cell_type":"code","source":"n = np.linspace(0, 1, 1000)\nx1 = np.sin(2 * np.pi * 100 * n)\nx2 = np.sin(2 * np.pi * 400 * n)\nx3 = np.concatenate((x1, x2))","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First, let's look at the DFT of our signals.","metadata":{}},{"cell_type":"code","source":"freqs_1 = centered_magnitude_spectrum(x1)\nfreqs_2 = centered_magnitude_spectrum(x2)\nfreqs_3 = centered_magnitude_spectrum(x3)\n\nfig, axs = plt.subplots(1, 3, figsize=(16, 4), dpi=200)\n\naxs[0].plot(np.linspace(-500,500, len(freqs_1)), freqs_1)\naxs[0].set_ylabel('DFT Magnitude')\naxs[0].set_xlabel('Frequency [Hz]')\naxs[0].set_title(\"100 Hz Signal\")\n\naxs[1].plot(np.linspace(-500,500, len(freqs_2)), freqs_2)\naxs[1].set_ylabel('DFT Magnitude')\naxs[1].set_xlabel('Frequency [Hz]')\naxs[1].set_title(\"400 Hz Signal\")\n\naxs[2].plot(np.linspace(-500,500, len(freqs_3)), freqs_3)\naxs[2].set_ylabel('DFT Magnitude')\naxs[2].set_xlabel('Frequency [Hz]')\naxs[2].set_title(\"100 Hz and 400 Hz Signal\")\nplt.show()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As expected, the *pure tones* (the 100 Hz and 400 Hz sine waves) have 2 peaks each, whereas the signal formed by concatenating them has 4 peaks. Now, let's look at the spectrograms of these signals. Run the following code to plot the spectrogram of each signal.","metadata":{}},{"cell_type":"code","source":"f1, t1, x1_freqs = signal.spectrogram(x1, fs=1000)\nf2, t2, x2_freqs = signal.spectrogram(x2, fs=1000)\nf3, t3, x3_freqs = signal.spectrogram(x3, fs=1000)\n\nfig, axs = plt.subplots(1, 3, figsize=(16, 4), dpi=200)\n\naxs[0].pcolormesh(t1, f1, x1_freqs, cmap=\"gray\", shading='auto')\naxs[0].set_ylabel('Frequency [Hz]')\naxs[0].set_xlabel('Time [sec]')\naxs[0].set_title(\"100 Hz Signal\")\n\naxs[1].pcolormesh(t2, f2, x2_freqs, cmap=\"gray\", shading='auto')\naxs[1].set_ylabel('Frequency [Hz]')\naxs[1].set_xlabel('Time [sec]')\naxs[1].set_title(\"400 Hz Signal\")\n\naxs[2].pcolormesh(t3, f3, x3_freqs, cmap=\"gray\", shading='auto')\naxs[2].set_ylabel('Frequency [Hz]')\naxs[2].set_xlabel('Time [sec]')\naxs[2].set_title(\"100 Hz and 400 Hz Signal\")\n\nplt.show()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The first spectrogram has a single band at 100 Hz. The second has a single band at 400 Hz. The final one has two bands (one at 100 Hz and one at 400 Hz). The reason we aren't seeing conjugate symmetry here is because we are only plotting the positive frequencies. For the most part, these spectrograms appear to give us the same information as the DFT. \n\nHowever, notice that in the 3rd spectrogram, the frequencies are mostly only present for the duration they exist. There's some overlap between 1.0-1.2 seconds, which isn't what we would have expected. This happens because SciPy doesn't truly use distinct chunks, as we mentioned above, and instead goes with a more sophisticated overlapping window approach, covered in EE 123 (this gives a better tradeoff between the temporal and spectral resolutions).","metadata":{}},{"cell_type":"markdown","source":"## Q1c: Spectrograms of Songs\n\nNow that we've got the basic concepts down, let's load *Viva La Vida* and *Mr. Brightside* and compare their spectrograms. Run the cell below to load the songs.","metadata":{}},{"cell_type":"code","source":"fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\ncoldplay = np.mean(coldplay, axis=1)\n\nfs, killers = wavfile.read(\"MrBrightside.wav\")\nkillers = np.mean(killers, axis=1)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since we haven't heard *Mr. Brightside* yet, let's load it in now and have a listen. This cell will take a few seconds to load before the audio interface shows up.","metadata":{}},{"cell_type":"code","source":"ipd.Audio(\"MrBrightside.wav\")","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To get a better looking image when visualizing the spectrogram, we'll plot everything in decibels.\n\nA decibel is a logarithmic unit of measuring sound, and is often used for visualization in cases like this where we have a large signal range. You might see this abbreviated as dB. In future classes, you will find that dB can also be used for logarithmic measurement of other types of waveforms (for example, in circuits).\n\nTo convert a number $x$ to decibels, we compute $x_\\text{dB} = 20\\log_{10}(x).$","metadata":{}},{"cell_type":"markdown","source":"### Your Job\n\nIn the cell below:\n1. Use [`signal.spectrogram`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html) to compute the spectrogram of each song. \n    - Use 4096 for the `nperseg` parameter of `signal.spectrogram` to take a 4096 point DFT. This matches the length of DFT typically used in practical audio fingerprinting systems, representing a good tradeoff between spectral and temporal resolution.\n    - The function returns a tuple containing the frequencies of the spectrogram samples, time points of the spectrogram samples, and the actual spectrogram. **Make sure you return these in the same order they are provided by `signal.spectrogram`!**.\n2. Convert the resultant spectrograms to the decibel scale using the formula from above.\n\n**To ensure there are no divide by zero warnings and to make sure the spectrogram renders properly, please add `epsilon_db_constant` (a small positive constant) before taking the log when converting to decibels.**\n\n*Hint 1*: Make sure you use `np.log10` in your computations\n<br>\n*Hint 2*: Don't forget to pass in the sampling frequency, `fs`, into the spectrogram!","metadata":{}},{"cell_type":"code","source":"def compute_spectrogram(fs, audio, epsilon_db_constant):\n    \"\"\"\n    Input:\n    fs - the sampling frequency of the audio, in Hertz (Hz)\n    audio - the full audio to compute the spectrogram of; either coldplay or killers\n    epsilon_db_constant - a small positive constant to ensure there are no divide by zero errors\n    \n    Output (np.ndarray, np.ndarray, np.ndarray):\n    Returns a scipy spectrogram for the given audio (in decibels) with three components:\n     - a NumPy array of sample frequencies\n     - a NumPy array of segment times\n     - the spectrogram itself\n    \n    See:\n    scipy.signal.spectrogram\n    numpy.log10\n    \"\"\"\n    \n    # TODO YOUR CODE HERE\n\n    ...","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's have a look! Run the cell below to compute spectrograms for both *Viva La Vida* and *Mr. Brightside*, then plot their spectrograms.","metadata":{}},{"cell_type":"code","source":"# Compute spectrogram for Viva La Vida\nf1, t1, coldplay_spect = compute_spectrogram(fs, coldplay, epsilon_db_constant=1e-12)\n\n# Compute spectrogram for Mr. Brightside\nf2, t2, killers_spect = compute_spectrogram(fs, killers, epsilon_db_constant=1e-12)\n\nplt.figure(figsize=(20, 10), dpi=200)\n\nplt.subplot(2, 1, 1)\nplt.pcolormesh(t1, f1, coldplay_spect, cmap=\"jet\", shading=\"auto\")\nplt.ylabel(\"Frequency [Hz]\")\nplt.xlabel(\"Time [sec]\")\nplt.title(\"Viva La Vida\")\nplt.colorbar()\n\nplt.subplot(2, 1, 2)\nplt.pcolormesh(t2, f2, killers_spect, cmap=\"jet\", shading=\"auto\")\nplt.ylabel(\"Frequency [Hz]\")\nplt.xlabel(\"Time [sec]\")\nplt.title(\"Mr. Brightside\")\nplt.colorbar()\n\nplt.show()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Q:** In both spectrograms, we see a column of dark blue for the first second or so. Based on our colorbar, it looks like this corresponds to $\\approx -300 \\text{dB}$, or essentially no signal power. In terms of the songs, why do we have this in our plots?","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color:blue\">**A:** (TODO) </span>","metadata":{}},{"cell_type":"markdown","source":"**Q:** At the beginning of the spectrogram for Mr. Brightside (after the column of dark blue), you should see two peaks that extend up toward $20 \\text{ kHz}$. What sound in the song is this part of the spectrogram capturing?","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color:blue\">**A:** (TODO) </span>","metadata":{}},{"cell_type":"markdown","source":"**Q:** Can you easily tell the two songs' spectrograms apart? Do you think they'd make good building blocks for our audio recognition algorithm?","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color:blue\">**A:** (TODO) </span>","metadata":{}},{"cell_type":"markdown","source":"# Q2: Fingerprinting\n\nOur end goal here is to take an audio snippet and figure out what song's being played. To do this, we'll need a database of songs to compare against. \n\nShould we just store entire songs in the database? Probably not, as that'd be a very large database: a three-minute WAV file sampled at $48 \\text{ kHz}$ is a about $30 \\text{ MB}$ in size. Even if we aimed for the modest goal of 1000 songs (which the original iPod from 2001 could hold), we're already looking at using over $30 \\text{ GB}$ of storage. Additionally, comparing raw audio samples for similarity isn't very robust against noise.\n\nInstead, we'll generate a set of *fingerprints* from each song, and store these in our database. When our version of Shazam gets fed a song to classify, it can just compare the fingerprints, rather than looking at the whole song. This should solve our storage issues, provided the fingerprints aren't too large. But clearly we'll need this fingerprinting algorithm to have a few other properties for this audio recognition system to be useful.\n\nIn particular, we want our audio fingerprint to have four key properties:\n1. ***Temporal Locality:*** We're trying to figure out what song is being played based on a short (say, 5 to 10 second long) clip. So, our fingerprints should somehow encode *where* in the song they come from.\n\n2. ***Translational Invariance:*** The snippet we play for Shazam could come from anywhere in the song. We could play it the first 5 seconds, the last 5, or something in the middle. In all cases, we want a correct result, so the same chunk of audio should get the same fingerpint regardless of whether it shows up a minute into a clip or right at the beginning—it's the actual music in it that we should use to generate the fingerprint.\n\n3. ***Robustness:*** An audio file, whether clean or degraded by (a modest amount of) noise, should produce the same fingerprint.\n\n4. ***High Entropy:*** The fingerprinting algorithm should be \"random enough\" that two different songs don't produce the same fingerprint.\n\nAs it turns out, spectrograms have all these nice properties, which is why they're such an important part of Shazam! The company's founders recognized this too, and discussed it in their original paper, linked in the references.","metadata":{}},{"cell_type":"markdown","source":"**So, spectrograms are cool, but how can we use them? They contain thousands of points... how do we pick which are the most important?**\n\nAs you might guess, we'll look at the spectrogram's *peaks*: points in high-energy areas. These are the most likely to survive distortions from noise, unlike ones that are close to zero and easily drowned out.","metadata":{}},{"cell_type":"markdown","source":"## Q2a: Peak Finding\n\nTo extract these peaks, we want to find areas of the spectrogram where's there's some point that has more energy than its neighbors. To do this, we're going to need some filtering. \n\n### Max Filtering \n\nTo do our peak finding, we'll use Scipy's [`maximum_filter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.maximum_filter.html) function.\n\nFor each point in our spectrogram, this filter will take our spectrogram $f(x, y)$ and output $g(x, y)$, the maximum value in a 51x51 region around the pixel. \n\nFormally,\n\n$$g(x, y) = \\max_{i,j} f(x+i, y+j) \\text{  where } -25\\le i, j \\le 25.$$\n\n### Your Job\n\n1. Implement the maximum filter and apply it to the provided spectrogram. When the neighborhood exceeds the boundary of the image, assume $f(x, y)$ is the value of the image at that point (i.e., set `mode='constant'` and `size=neighborhood_size`).\n2. Extract a boolean mask which is True when $f(x, y) = g(x, y)$, and False otherwise.\n3. To ensure these peaks are big enough, in the mask, set any peak locations with a peak less than or equal to `AMP_THRESH` to zero. This is filled in for you. \n4. Use [`np.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) to convert your mask into a set of (frequency, time) pairs. This function will return two arrays. The first is the indices along the frequency axis of the spectrogram where the peaks show up, and the second is the peak indices along the time axis.","metadata":{}},{"cell_type":"code","source":"def peak_finding(spect, neighborhood_size=2*25+1, amp_thresh=40):\n    \"\"\"\n    Input:\n    spect - the spectrogram of an unknown audio track to find peaks from\n    neighborhood_size - the size of the maximum filter\n    amp_thresh - amplitude threshold to include peaks in result\n    \n    Output (np.ndarray, np.ndarray):\n    Returns a tuple of the peak indices on the frequency \n    and time axes (each as NumPy arrays) for the provided spectrograph.\n    \n    See:\n    maximum_filter\n    np.nonzero\n    \"\"\"\n\n    # Apply a Maximum Filter\n    max_spect = ...\n\n    # Compute the mask\n    mask = ...\n\n    # Filter out tiny peaks\n    mask &= spect > amp_thresh\n\n    # Get the indices of the peaks\n    freq_indices, time_indices = ...\n\n    return freq_indices, time_indices\n\n# Call peak_finding with the spectrogram for Viva La Vida\nfreq_indices, time_indices = ...","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grader.check(\"q2a\")","metadata":{"deletable":false,"editable":false,"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 6), dpi=200)\nplt.scatter(t1[time_indices], f1[freq_indices], zorder=99, color='k')\nplt.pcolormesh(t1, f1, coldplay_spect, zorder=0, cmap=\"jet\", shading=\"auto\")\nplt.ylabel('Frequency [Hz]')\nplt.xlabel('Time [sec]')\nplt.title(\"Spectrogram Peaks (for Viva La Vida)\")\nplt.xlim([0, 60])\nplt.show()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Q:** In Q1, we saw how most of the information in music signals is in the lower frequencies (under, say, $10 \\text{ kHz}$). How does this compare with the spectrogram peaks? Are they mostly in lower or upper half of the spectrgram? Is this what you'd expect?","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color:blue\">**A:** (TODO)</span>","metadata":{}},{"cell_type":"markdown","source":"## Q2b: Fingerprinting","metadata":{}},{"cell_type":"markdown","source":"The peaks we've found make up what the creators of Shazam call a *constellation map*. We'll use the points in our constellation map to compute the song's fingerprints. \n\nTo do this, we'll take each peak, say $(t_i, f_i)$, and chain it together with the next $n$ peaks $(t_{i+1}, f_{i+1}), ..., (t_{i+n}, f_{i+n})$ by hashing the values of the peaks. Hashing is out of the scope of this course, but at a high level hashing is a technique that transforms any given key or string into a (essentially) unique hash, or fingerprint. We've provided a function `hashing(f1, t1, freq_indices, time_indices)` which returns a list of hashes for the provided parameters.\n\nAfter fingerprinting, all we need to do is search our database for a match. If we did things correctly, the database entry we have the most fingerprints in common with should match the true song.\n\nLet's move all of this code into a single function so we can easily compute hashes for any audio signal.\n\n`fingerprint` should return an array of tuples, each one containing the hash $h$ and the time $t_i$.\n\n*Note:* Don't convert the spectrogram to decibels in this part! We converted the spectrogram to decibels in earlier parts for ease of rendering, but there's no need to do that here (and converting to decibels will cause you to fail the tests). This also means **DO NOT USE your implementation of `compute_spectrogram` from Q1c**.\n\n*Hint 1:* We specify that you should not use `compute_spectrogram` but it may be useful to look into that function\n<br>\n*Hint 2:* When calling `peak_finding` make sure to pass in `neighborhood_size` and `amp_thresh`","metadata":{}},{"cell_type":"code","source":"def fingerprint(fs, audio, neighborhood_size=2*25+1, amp_thresh=40):\n    \"\"\"\n    Input:\n    fs - the sampling frequency of the audio, in Hertz (Hz)\n    audio - the full audio to fingerprint; either coldplay or killers\n    neighborhood_size - the size of the maximum filter\n    amp_thresh - amplitude threshold to include peaks in result\n    \n    Output (list[str, int]):\n    A list of hashes representing the \"fingerprint\" of the given audio.\n    \"\"\"\n    audio = np.mean(audio, axis=1)\n    \n    # Compute the spectrogram of the single channel audio\n    f1, t1, spect = ...\n\n    # Find the peaks (Use function from Q2a)\n    freq_indices, time_indices = ...\n    \n    # Compute the hashes\n    hashes = ...\n    \n    # Return list of hashes\n    return hashes","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grader.check(\"q2b\")","metadata":{"deletable":false,"editable":false,"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Q3: Testing\nAs mentioned before, all we need to do now is test our system and make sure it's as robust as we think it is. Our database is stored in `database.csv`. It's columns are |Hash|t1|Song|. A production application with thousands of songs in the database would use SQL or some other querying language, but a simple CSV will suffice for our uses.\n\nBecause searching through our database is more of a software problem than a Signals and Systems problem, we've provided the detection function for you. \n\nThis function:\n1. Loads the CSV using pandas (a data analysis package),\n2. Fingerprints the unknown sample,\n3. Searches for matches, and\n4. Returns the song with the most matches, its confidence as a percentage.","metadata":{}},{"cell_type":"code","source":"def detect(fs, audio):\n    db = pd.read_csv(\"database.csv\", header=None, names=[\"Hash\", \"time\", \"Song\"])\n\n    hashes = fingerprint(fs, audio)\n    db_matches = db[db.Hash.isin(map(lambda x: x[0], hashes))]\n    if len(db_matches) == 0:\n        print(\"No Matches\")\n        return\n\n    counts = db_matches.groupby(\"Song\").size()\n    counts = counts / counts.sum()\n    return counts.idxmax(), counts.max() * 100","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Q3a: Segmenting Audio\n\nShazam usually only has a few seconds of data to work with, so we will as well. Start by writing a function to take a 20 second segment from either Viva La Vida or Mr. Brightside. The specific start and end times don't matter too much, just remember both audio tracks are only 60 seconds long!\n\n*Hint:* You've probably seen this operation performed several times already in this lab","metadata":{}},{"cell_type":"code","source":"def get_20_second_segment(fs, audio):\n    \"\"\"\n    Input:\n    fs - the sampling frequency of the audio, in Hertz (Hz)\n    audio - the full audio to get 20 seconds of; either coldplay or killers\n    \n    Output:\n    A 20 second segment anywhere within the given audio track.\n    \n    Example:\n    get_20_second_segment(killers) == killers[X seconds:(X + 20 seconds)]\n    \"\"\"\n    \n    ...","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grader.check(\"q3a\")","metadata":{"deletable":false,"editable":false,"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You will now use this function to write tests for your Shazam system!","metadata":{}},{"cell_type":"markdown","source":"## Q3b: Basic Testing\n\nLet's see how our system does under ideal conditions (i.e, no noise). Take a 20 second segment from Viva La Vida and Mr. Brightside and call the `detect` function to identify it. We've already reloaded the audio for you.","metadata":{}},{"cell_type":"code","source":"fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\nfs, killers = wavfile.read(\"MrBrightside.wav\")","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def basic_detect_test(fs, audio):\n    \"\"\"\n    Input:\n    fs - the sampling frequency of the audio, in Hertz (Hz)\n    audio - the full audio to detect against; either coldplay or killers\n    \n    Output:\n    Returns the name of the audio track that most closely matches \n    a 20 second segment of the provided audio track, and a percentage confidence.\n    \n    Example:\n    basic_detect_test(killers_fs, killers) == ('MrBrightside.wav', 100.0)\n    \n    See also:\n    get_20_second_segment\n    detect\n    \"\"\"\n    \n    ...\n    ","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grader.check(\"q3b\")","metadata":{"deletable":false,"editable":false,"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Q3c: Gaussian Noise\nWe want our system to be robust to different forms of noise. To start with, lets add some Gaussian noise to our audio and try to detect its origin. Take a 20 second chunk of Viva La Vida, add Gaussian noise with a mean and variance of 10000, and see if you can identify them. \n\n*Hint 1*: Checkout the [`np.random.normal`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html) function. \n<br>\n*Hint 2*: Make sure to pass the `size` parameter to `np.random.normal`\n<br><br>\n*Note*: the tests for this section will add random noise to both songs and check that the `detect` function still classifies them correctly.","metadata":{}},{"cell_type":"code","source":"NOISE_MEAN = 10000\nNOISE_STANDARD_DEVIATION = 10000\n\ndef add_gaussian_noise(audio_segment):\n    \"\"\"\n    Input:\n    audio_segment - an audio segment from an unknown track\n    \n    Output:\n    Returns the audio segment with added Gaussian noise.\n    \n    See:\n    Problem description (for quantities)\n    np.random.normal\n    \"\"\"\n    \n    # TODO YOUR CODE HERE\n\n    ...","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gaussian_noise_detect_test(fs, audio_segment):\n    \"\"\"\n    Input:\n    fs - the sampling frequency of the audio, in Hertz (Hz)\n    audio_segment - an audio segment from an unknown track WITHOUT Gaussian noise\n    \n    Output:\n    Returns the name of the audio track that most closely matches \n    a 20 second segment of the provided audio track, WITH added \n    Gaussian noise and a percentage confidence.\n    \n    See:\n    add_gaussian_noise\n    detect\n    \"\"\"\n    \n    # TODO YOUR CODE HERE\n\n    ...","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ipd.Audio(add_gaussian_noise(get_20_second_segment(fs, coldplay)).T, rate=fs)","metadata":{"tags":["otter_answer_cell"],"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grader.check(\"q3c\")","metadata":{"deletable":false,"editable":false,"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our version of Shazam should still be able to detect the song. How does it sound, though?","metadata":{}},{"cell_type":"markdown","source":"It sounds terrible, and we can barely make out the music! Yet, our system still correctly identified it as *Viva La Vida*!","metadata":{}},{"cell_type":"markdown","source":"## Q3d: Blocked Speaker\n\nWhat if instead of Gaussian noise, a portion of the audio just becomes zero? Arguably, this is a more realistic model of how our signal could get corrupted when dealing with music recognition. For example, somebody could move in front of the speaker, pause the music, or turn the volume down very low. \n\nLet's take a 20 second chunk of Viva La Vida, zero out five 2 second chunks, and see if we can still detect the source.\n\nYou don't need to implement any code here — just run the cells.","metadata":{}},{"cell_type":"code","source":"unknown_segment = coldplay[10 * fs: 30 * fs].copy()\nunknown_segment[:2 * fs] = 0\nunknown_segment[6 * fs:8 * fs] = 0\nunknown_segment[16 * fs:20 * fs] = 0\nunknown_segment[2 * fs:4 * fs] = 0","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's hear how the song sounds with these portions removed.","metadata":{}},{"cell_type":"code","source":"ipd.Audio(unknown_segment.T, rate=fs)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"How does Shazam do now? Surely it'll fail with half the clip missing.","metadata":{}},{"cell_type":"code","source":"detect(fs, unknown_segment)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Again, it succeeds! Our fingerprinting procedure is again proving its robustness. What about *Mr. Brightside*?","metadata":{}},{"cell_type":"code","source":"unknown_segment = killers[10 * fs: 30 * fs].copy()\nunknown_segment[:2 * fs] = 0\nunknown_segment[6 * fs:8 * fs] = 0\nunknown_segment[16 * fs:20 * fs] = 0\nunknown_segment[2 * fs:4 * fs] = 0","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ipd.Audio(unknown_segment.T, rate=fs)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detect(fs, unknown_segment)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looks like our system is pretty robust!","metadata":{}},{"cell_type":"markdown","source":"<a id='final-check'></a>\n## Final Autograder Check\nIf you have passed all of the autograder checks above, the cell below will output \"All tests passed!\". If you see this message when you run the cell below, please fill out the checkoff Google form linked below in the Checkoff section. \n\nPlease do not modify the cell below!","metadata":{}},{"cell_type":"code","source":"# === Run all tests ===\nimport otter\n\ngrader = otter.Notebook()\nresults = grader.check_all()     \n\nprint(results.summary())\n\ntry:\n    all_ok = results.passed_all_public  \nexcept AttributeError:\n    d = results.to_dict() if hasattr(results, \"to_dict\") else {}\n    tests = d.get(\"results\", []) or d.get(\"test_file_results\", [])\n    all_ok = bool(tests) and all(t.get(\"passed\", False) for t in tests)\n\nprint(\"\\n✅ All tests passed!\" if all_ok else \"\\n❌ Some visible tests failed. Scroll up for details.\")\n","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='feedback'></a>\n## Feedback\nIf you have any feedback to give the teaching staff about the course (lab content, staff, etc), you can submit it through this Google form. Responses are **fully anonymous** and responses are actively monitored to improve the labs and course. Completing this form is **not required**.\n\n[Anyonymous feedback Google form](https://docs.google.com/forms/d/e/1FAIpQLSch7s4OhheMytSjo3qPfyBv9_U8IBZPm0Syi1qGaKPANGNHFw/viewform)\n\n*If you have a personal matter to discuss or need a response to your feedback, please contact <a href=\"mailto:eecs16a.lab@berkeley.edu\">eecs16a.lab@berkeley.edu</a> and/or <a href=\"mailto:eecs16a@berkeley.edu\">eecs16a@berkeley.edu</a>*.","metadata":{}},{"cell_type":"markdown","source":"<a id='checkoff'></a>\n## Checkoff\nTo receive credit, all labs will require the submission of a checkoff Google form. This link will be at the bottom of each lab. Both partners should fill out the form (you should have one submission per person), and feel free to use the same Google account/computer to fill it out as long as you have the correct names and student IDs.\n\n[Fill out the checkoff Google form.](https://docs.google.com/forms/d/e/1FAIpQLScwrFoYRPPZ7eAhCnDLsIB7FFP2na2CgkW0RTa3A0Ii1Xf5NA/viewform)","metadata":{}},{"cell_type":"markdown","source":"# Final Comments (Optional)\nThere are many ways to improve our Shazam system. Many of them have to do with how we compute our spectrogram as well as the various parameters we introduced such as `NEIGHBORHOOD_SIZE`, `AMP_THRESH`, and `HASHES_PER_PEAK`. But, for the most part, this is how Shazam works!\n\nThe original Shazam paper uses a different method for matching the fingerprints of audio instead of a simple \"most matches => song\" scheme, but for our limited database, this works just fine. Check out the original paper if you are curious. If you'd like, you can use the following cells to load your own songs into the database (as long as they are wav files) and try to identify samples of them.","metadata":{}},{"cell_type":"code","source":"import csv\ndef add_to_db(filename):\n    fs, audio = wavfile.read(filename)\n    hashes = fingerprint(audio, fs)\n    with open('database.csv', mode='a') as db_file:\n        db_writer = csv.writer(db_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n\n        for hash_pair in hashes:\n            db_writer.writerow([hash_pair[0], hash_pair[1], filename])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run this to add a song to the database.","metadata":{}},{"cell_type":"code","source":"my_wav_filepath = ___  # Path to any WAV file you want to add to the database\nadd_to_db(my_wav_filepath)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Then run this to detect against your updated database. Inputting the same WAV file should result in your new WAV file being detected! Try with a WAV file that isn't in the database, or a WAV file that is a composite of multiple songs in the database. Experiment and see what happens!","metadata":{}},{"cell_type":"code","source":"fs, audio = wavfile.read(___)  # Path to any WAV file to detect against\ndetect(fs, audio)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# References\n\n[1] *An industrial strength audio search algorithm.* [[Link](http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf)].  \n[2] *Audio fingerprinting with Python and Numpy.* [[Link](https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/)].","metadata":{}}]}